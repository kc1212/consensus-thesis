\section{Implementation}
\label{sec:implementation}

The prototype implementation can be found on GitHub.
\begin{displayquote}
\url{https://github.com/kc1212/consensus-thesis-code}
\end{displayquote}
It implements the three protocols and the Extended TrustChain discussed in~\Cref{ch:model}.
We also implement two optimisations---privacy preserving validation protocol using compact blocks (\Cref{sec:compact})
and optimised validation protocol using cached agreed fragments (\Cref{sec:caching}).
It is written in the event driven paradigm, using the Python programming language\footnote{\url{https://www.python.org/}}.
We use the Twisted\footnote{\url{https://twistedmatrix.com/}} library for networking.

Every node keeps a persistent TCP connection with every other node.
This creates a fully connected network for our experiment.
It is certainly not ideal in real world scenarios where nodes may have limited resources (e.g. sockets).
But as a prototype, it is sufficient to run a network of over a thousand nodes and experiment with it.

Finally, the cryptography primitives we use are SHA256 for hash functions and Ed25519 for digital signatures.
Both of which are provided by libnacl~\footnote{\url{https://pypi.python.org/pypi/libnacl}}.


\subsection{Experimental setup}
\label{sec:experimental-setup}

The goal of the experiment is to run the three protocols---consensus protocol,
transaction protocol and validation protocol---simultaneously and analyse the communication cost, the consensus duration and the global throughput.
We investigate these properties under the following parameters.
2 TX/s
\begin{enumerate}
  \item The number of facilitators $n \in \{4, 8, \dots, 32\}$.
        The maximum $n$ is 32 because the limitation in liberasurecode mentioned in \Cref{sec:implementation},
        but our results give a good indication of how our system may function for larger numbers of $n$.
  \item The population size $N \in \{200, 300, \dots, 1200\}$.
        $N$ stops at 1200 is due to our physical setup, which we describe next.
\end{enumerate}

The experiment is run on the DAS-5\footnote{\url{https://www.cs.vu.nl/das5/}}.
From now on, we use ``machines'' to refer to DAS-5 nodes and ``nodes'' to refer to a running instance in our system.
On DAS-5 we use up to 30 machines, for each machine we use up to 40 nodes.
This gives us the aforementioned 1200 number.
With this setup, we cannot run more nodes because the every machine only has 65535 ports available (minus the reserved ones).
But 40 nodes each need 1200 TCP connections which is 48000 TCP connections per machine and that is inching close to the limit.
While it is possible to have more TCP connections per machine,
but it requires additional network interface which is something we do not control on the DAS-5.
Nevertheless, running the system with 1200 nodes gives a good indication of its scalability properties as we show later.

To coordinate nodes on many different machines,
we employ a discovery server to inform every node the IP addresses and port numbers of every other node.
It is only run before the experiment and is not used during the experiment.

Finally, since Bitcoin transactions are approximately 500 bytes~\cite{txsize},
we use a uniformly random transaction size sampled between 400 and 600 bytes.