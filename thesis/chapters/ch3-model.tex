\chapter{System architecture}
\label{ch:model}

% As was shown in~\Cref{ch:problem}, the requirements are the following.
% \begin{itemize}
%     \item Global consensus
%     \item Security and fault tolerance
%     \item Performance and scalability
% \end{itemize}

% As mentioned in the Introduction, the primary goal is performance and scalability.
% Having a scalable blockchain system while still keeping global consensus
% allows the system to be ubiquitous and realise the full potential of blockchain.

% The secondary goal is to design an application neutral system.
% In particular, it should act as a backbone that provides the building blocks of blockchain based applications.
% It should be possible to buld any kind of application on top our system.
% Further, we do not impose on a consensus algorithm, as long as it satisfies the properties of atomic broadcast which we describe in~\Cref{sec:overview-cons}.


% The third and final goal is security.
% Our system should be unaffected in the presence of powerful adversaries.
% In particular, adversaries are Byzantine meaning that they can have arbitrary behaviour.
% Thus anything is possible from simply ommitting messages to colluding with eachother in order to undermine the whole system.

We present a design which removes the transaction rate restrictions seen in traditional blockchain systems.
This is done by combining the recent HoneyBadgerBFT~\cite{miller2016honey}
(a purely asynchronous consensus algorithm for blockchain systems)
with a modification of our prior work on TrustChain~\cite{trustchain}.
The design is inspired by (the distributed part of) the Internet architecture as well as how transactions are performed in the real world.

The goal of this chapter is to detail our system specification.
We begin our discussion with an intuitive overview of the architecture in \Cref{sec:system-overview}.
Next, we give the formal description, starting with the model and assumptions in \Cref{sec:model-assumptions}.
Then, the three protocols which make up the complete system,
namely consensus protocol (\Cref{sec:cons-protocol}), transaction protocol (\Cref{sec:tx-protocol}) and validation protocol (\Cref{sec:vd-protocol}).
Finally, we discuss a few variations of our main design and their respective tradeoffs in \Cref{sec:tradeoffs}.

\section{System overview}
\label{sec:system-overview}
The system consist of one data structure---Extended TrustChain,
and three protocols---consensus protocol, transaction protocol and validation protocol.
We first describe each component individually and then explain how they fit together in \Cref{sec:combined-protocol}.

\subsection{Extended TrustChain}
Extended TrustChain naturally builds on top of the standard TrustChain. 
Thus we first describe the standard TrustChain.
Our description has minor differences compared to the description in~\cite{trustchain}.
This is to help with the description of the extended TrustChain.
We remark the difference when it occurs.
However, the two descriptions are functionally the same.

\subsubsection*{Standard TrustChain}
In TrustChain, every node has a ``personal'' chain. 
Initially, the chain only contains a genesis block generated by the nodes themselves.
When a node $A$ wishes to add a new transaction (TX) with $B$, a new TX block is generated and then appended to $A$'s chain.
The TX block must have a valid hash pointer pointing to the previous block
and a reference\footnote{This is different from the original TrustChain definition found in~\cite{trustchain}.
In there, a TX block has two outgoing edges which are hash pointers to the two parties involved in the transaction.
This work uses one outgoing edge and a reference.} to its \emph{pair} on $B$'s chain.
As a result, a single transaction generates two TX blocks, one on each party's chain.
An example of is shown in \Cref{fig:trustchain-bad}.

\begin{figure}
    \includegraphics[width=0.9\textwidth]{trustchain-bad}
    \centering
    \caption{Every block is denoted by $t_{i,j}$, where $i$ is the node ID and $j$ is the sequence number of the block.
    Thus we have three nodes and three corresponding chains in this example.
    The arrows represent hash pointers and the dotted lines represent references.
    The blocks at the ends of one dotted line are pairs of each other.
    The red block after $t_{b, 5}$ indicate a fork.}
    \label{fig:trustchain-bad}
\end{figure}

If every node follows the rules of TrustChain and we only consider hash pointers,
then every chain effectively forms a singly linked list.
However, if a node violates the rules, then a \emph{fork} may happen.
That is, there may be more than one TX block with a hash pointer pointing back to the same block.
In \Cref{fig:trustchain-bad}, node $b$ (in the middle chain) created two TX blocks that both point to $t_{b, 5}$.
If this is a ledger system it can be seen as a double spend, where the currency accumulated up until $t_{b, 5}$ are spent twice.

\subsubsection*{Extended TrustChain}
We are now ready to explain the Extended TrustChain.
The primary difference is the introduction of a new type of block---checkpoint (CP) block.
In contract to TX blocks, CP blocks do not store transactions or contain references.
Their purpose is to capture the state of the chain and the state of the whole system.
In particular, the state of the chain is captured with a hash pointer.
The state of the whole system is captured in the content of the CP block,
namely as a digest of the latest \emph{consensus result} which we explain in \Cref{sec:overview-cons}.
A visual representation is shown in \Cref{fig:trustchain-bad-cp}.

From this point onwards, we use TrustChain to mean the Extended TrustChain unless explicitly clarified.

\begin{figure}
    \includegraphics[width=0.9\textwidth]{trustchain-bad-cp}
    \centering
    \caption{The circles represent CP blocks,
    they also have hash pointers (arrow) but do not have references (dotted line).
    Note that the sequence number counter do not change, it is shared with TX blocks.}
    \label{fig:trustchain-bad-cp}
\end{figure}

\subsection{Consensus protocol}\label{sec:overview-cons}

% The consensus protocol uses an existing Byzantine consensus algorithm (explained in detail in \Cref{sec:xyz}) and runs it continuously in rounds.
The consensus protocol can be seen as a technique of running infinitly many rounds of some 
Byzantine consensus algorithm\footnote{More accuratly it is ACS or asynchronous subset consensus, we describe ACS in \Cref{sec:acs-background}.},
starting a new execution immediately after the previous one is completed.
This is necessary because blockchain systems always need to reach consensus on new values proposed by nodes in the system,
or CP blocks in our case.

The high communication complexity of Byzantine consensus algorithms prohibits us from running it on a large network.
Thus, for every round, we randomly select some node---called facilitators---to collect CP blocks from every other node and use them as the proposal.
The facilitators are elected using a \emph{luck value}, which is computed using $\textsf{H}(\C_r || pk_u)$,
where $\C_r$ is the consensus result 
(which can be seen as the set union of all the CP blocks collected by the facilitators)
in round $r$ and $pk_u$ is the public key of node $u$.
Intuitively, the election is guaranteed to be random 
because the output of a cryptographically secure hash function is unpredictable and $\C_r$ cannot be determined in advance.

A visual explaination can be found in \Cref{app:consensus-example},
it walks through the steps needed for a node to be selected as a facilitator using an example.

\subsection{Transaction and validation}
The TX protocol is a simple request and response protocol.
The nodes exchange one round of messages and create new TX blocks on their respective chains.
Thus, as we mentioned before, one transaction should result in two TX blocks.

The consensus and transaction protocol by themselves do not provide a mechanism to detect forks or other forms of tamperaing.
Thus we need a validation protocol to counteract malicious behaviour.
When a node wish to validate one of its TX, it asks the counterparty for the \emph{fragment} of the TX.
A fragment of a TX is a section of the chain beginning and ending with CP blocks that contains the TX and are in consensus.
Upon the counterparty's response, the node checks that the CP blocks are indeed in consensus,
the hash pointers are valid and his TX is actually in the fragment.
The TX is valid if these conditions are satisfied.
Intuitively, this works because it is hard (because a cryptographically secure hash function is second-preimage resistant)
to create a different chain that begins and ends with the same two CP blocks but with a different middle section.


\subsection{Combined protocol}
\label{sec:combined-protocol}
The final protocol is essentially the concurrent composition of the three aforementioned protocols,
all making use the Extended TrustChain data structure.

Our subprotocol design gives us the highly desireable non-blocking property.
In particular, we do not need to ``freeze'' the state of the chain for some communication to complete in order to create a block.
For instance, a node may start the consensus protocol, and while it is running, create new transactions and validate old transactions.
By the time the consensus protocol is done, the new CP block is added to whatever the state that the chain is in.
It is not necessary to lock the chain while the consensus protocol is running and then unlock it afterwards.

\section{Model and assumptions}
\label{sec:model-assumptions}

This section and the ones following it give a technical treatment of what the content in System Overview.
For notational clarity, we use the following convention (adapted from~\cite{miller2016honey}) throughout this work.
\begin{itemize}
\item Lower case (e.g. $x$) denotes a scalar object or a tuple.
\item Upper case (e.g. $X$) denotes a set or a constant.
\item Sans serif (e.g. $\textsf{fn}(\cdot)$) denotes a function.
\item Monospace (e.g. $\texttt{ack}$) denotes message type.
\end{itemize}
Further, we use $a || b$ to denote concatination of the binary representations of $a$ and $b$.

We assume purely asynchronous channels with eventual delivery.
Thus in no circumstance do we make timing assumptions.
The adversary has fully control of the delivery schedule and the message ordering of all messages.
But they are not allowed to drop messages except for their own\footnote{
    Reliability can be achieved in unreliable networks by resending messages or using some error correction code.
}.

We assume there exist a Public Key Infrastructure (PKI), and nodes are identified by their unique and permanent public key.
Finally, we use the random oracle model, i.e. calls to the random oracle are denoted by $\textsf{H}: \{0, 1\}^* \rightarrow \{0, 1\}^\lambda$,
where $\{0, 1\}^*$ denotes the space of finite binary strings and $\lambda$ is the security parameter~\cite{bellare1993random}.

In our model we consider $N$ nodes, which is the population size.
$n$ of them are facilitators, $t$ out of $n$ are malicious and the inequality
$n \ge 3t + 1$ must hold.
This is from the work of Pease, Shostak and Lamport, where they show a network of $n$ nodes cannot reach Byzantine agreement with $t \ge n/3$.
Further, the inequality $N \ge n + t$ must also hold.
This is due to our system design, which becomes clear in~\Cref{sec:consensus-phase}.

Our threat model is as follows. 
We use a restricted version of the adaptive corruption model.
The first restriction is that corrupted node can only change across rounds.
That is, if a round has started, the corrupted nodes cannot changed until the next round.
The second restriction is that the adversary, presumably controlling all the corrupted nodes, is forgetful.
Namely the adversary may learn the internal state such as the private key of a corrupted node,
but if the node recovers, then the adversary must forget the private key.
This is realistic because otherwise the adversary can eventually learn all the private keys and sabotage the system.
Finally, we assume computational security.
That is, the adversary can run polynomial-time algorithms but not exponential-time algorithms efficiently.

\section{Extended TrustChain}
The primary data structure used in our system is the Extended TrustChain.
Each node $u$ has a public and private key pair---$pk_u$ and $sk_u$, and a chain $B_u$.
The chain consist of blocks $B_u = \{ b_{u, i} : k \in \{ 0, \dots, h - 1 \} \}$,
where $b_{u, i}$ is the $i$th block of $u$,
and $h = |B_u|$.
We often use $b_{u, h}$ to denote the latest block.
There are two types of blocks, TX blocks and CP blocks.
If $T_u$ is the set of TX blocks of $u$ and $C_u$ is the set of CP blocks of $u$,
then it must be the case that $T_u \cup C_u = B_u$ and $T_u \cap C_u = \varnothing$.
The notation $b_{u, i}$ is generic over the block type.
We assume there exist a function $\textsf{typeof}: B_u \rightarrow \{ \tau, \gamma \}$ that returns the type of the block,
where $\tau$ represents the TX type and $\gamma$ represents the CP type.

\subsection{Transaction block}
The TX block is a six-tuple, i.e $t_{u, i} = \langle \textsf{H}(b_{u, i - 1}), seq_u, txid, pk_v, m, sig_u \rangle$.
We describe each item in turn.
\begin{enumerate}
\item $\textsf{H}(b_{u, i - 1})$ is the hash pointer to the previous block.
\item $seq_u$ is the sequence number which should equal $i$.
\item $txid$ is the transaction identifier, it should be generated using a cryptographically secure pseudo-random number generator by the initiator of the transaction.
\item $pk_v$ is the public key of the counterparty $v$.
\item $m$ is the transaction message.
\item $sig_u$ is the signature created using $sk_u$ on the concatination of the binary representation of the five items above.
\end{enumerate}
The fact that we have no constraint on the content of $m$ is in alignment with our design goal---application neutrality.

TX blocks come in pairs.
In particular, for every block 
$$t_{u, i} = \langle \textsf{H}(b_{u, i - 1}), seq_u, txid, pk_v, m, sig_u \rangle$$
there exist one and only one \emph{pair} 
$$t_{v, j} = \langle \textsf{H}(b_{v, j - 1}), seq_v, txid, pk_u, m, sig_v \rangle.$$
Note that the $txid$ and $m$ are the same, and the public keys refer to each other.
Thus, given a TX block, these properties allow us to identify its pair.

% TODO Node $v$ may cheat and create more than one pair for $t_{u, i}. we discuss later

\subsection{Checkpoint block}

The CP block is a five-tuple, 
i.e. $c_{u, i} = \langle \textsf{H}(b_{u, i-1}), seq_u, \textsf{H}(\C_r), r, sig_u \rangle$,
where $\C_r$ is the consensus result (which we describe in \Cref{sec:consensus-result}) in round $r$, the other items are the same as the TX block definition.
Note that unlike in our prior work~\cite{implicitconsensus}, CP blocks and TX blocks do not have independent sequence numbers.

The genesis block in the chain must be a CP block in the form of
$c_{u, 0} = \langle \textsf{H}(\bot), 0,  \textsf{H}(\bot), 0, sig_u \rangle$
where $\textsf{H}(\bot)$ can be interpreted as applying the hash function on an empty string.
The genesis block is unique because every node has a unique public and private key pair.


\subsection{Consensus result}
\label{sec:consensus-result}
Our consensus protocol runs in rounds as discussed in \Cref{sec:system-overview}.
Every round is identified by a round number $r$, which is incremented on every new round.
The consensus result is a tuple, i.e. $\C_r = \langle r, C \rangle$,
where $C$ is a set of CP blocks agreed by the facilitators of round $r$.

\subsection{Chain properties}
Here we define a few important properties which results from the interleaving nature of CP and TX blocks.

If there exist a tuple $\langle c_{u,a}, c_{u, b} \rangle$ for a TX block $t_{u, i}$,
where 
$$a = \argmin_{k, k < i, \texttt{typeof}(b_{u,k}) = \gamma}(i - k)$$
$$b = \argmin_{k, k > i, \texttt{typeof}(b_{u,k}) = \gamma}(k - i),$$
then $\langle c_{u,a}, c_{u, b} \rangle$ is the \emph{enclosure} of $t_{u, i}$.
Note that $c_{u, a}$ is the more recent CP block.
Also, some TX blocks may not have any subsequent CP blocks, then its enclosure is $\bot$.

If the enclosure of some TX block is $\langle c_{u,a}, c_{u, b} \rangle$,
then its \emph{fragment} is defined as $\{ b_{u, i} : a \le i \le b \}$.
For convenience, the function $\textsf{fragment}(\cdot)$ represents the fragment of some TX block if it exists, otherwise $\bot$.

\emph{Agreed enclosure} is the same as enclosure with an extra constraint where the CP blocks must be in some consensus result $\C_r$ and also must be the smallest enclosure.
That is, suppose a chain is in the form
    $\{c_{i}, c_{i+1}, t_{i+2}, c_{i+3}\}$\footnote{Usually the notation is of the form $c_{u, i}$, but the node identity is not important here so we simplify it to $c_{i}$}
    and $c_{i}, c_{i+1}, c_{i+3}$ are in $\C_r, \C_{r+1}, \C_{r+3}$ respectively,
    then the agreed enclosure of $t_{i+2}$ is $\langle c_{i+1}, c_{i+3}\rangle$ and cannot be $\langle c_{i}, c_{i+3}\rangle$.
Similarly, \emph{agreed fragment} has the same definition as fragment but using agreed enclosure.
We define its function to be $\textsf{agreed\_fragment}(\cdot)$ which we use later in the validation protocol (\Cref{sec:vd-protocol}).

% The length of the fragment is constrained by $L$,
% namely $\forall t |\textsf{fragment}(t)| \le L$.
% The purpose to prevent spam and encourage nodes to create more CP blocks.
% $L$ should be sufficiently high so that busy nodes are not hindered by it.


\section{Consensus protocol}
\label{sec:cons-protocol}

Our consensus protocol runs on top of Extended TrustChain.
It is directly related to the creation of CP blocks.
The objectives of the protocol are to
    allow honest nodes always make progress (in the form of creating new CP blocks),
    compute correct consensus result in every round
    and have unbiased election of facilitators.
Concretely, we define the necessary properties as follows.
\begin{definition}
\label{def:consensus}
\textbf{\emph{Properties of the consensus protocol}}

$\forall r \in \mathbb{N}$, the following properties must hold.
\begin{enumerate}
    \item \emph{Agreement}:
        If one correct node outputs a list of facilitators $\F_r$,
        then every node outputs $\F_r$
    \item \emph{Validity}:
        If any correct node outputs $\F_r$, then 
            \begin{enumerate}
                \item $|\C_r| \ge N - t$ must hold for the $\C_r$ which was used to create $\F_r$,
                \item $\F_r$ must contain at least $n - t$ honest nodes and
                \item $|\F_r| = n$.
            \end{enumerate}
    \item \emph{Fairness}:
        Every node with a CP block in $\C_r$ should have an equal probability of becoming a member of $\F_r$.
    \item \emph{Termination}:
        Every correct node eventually outputs some $\F_r$.
\end{enumerate}
\end{definition}
These properties may look like Byzantine consensus properties (which we describe next in \Cref{sec:acs-background}) but they have some subtle differences.
Firstly, they are properties for every node in the network and not just the facilitators.
Secondly, they must be satisfied for all rounds because the whole system falls apart if one of the property cannot be satisfied in one of the rounds.

Before describing the protocol in detail,
we take a brief detour to give background on the asynchronous subset consensus.
This is the primary building block of our protocol.
% Starting with the bootstrap phase and then moving on to the actual consensus phase.

\subsection{Background on asynchronous subset consensus}
\label{sec:acs-background}

The best way to explain asynchronous subset consensus (ACS) is to contrast it with the typical Byzantine consensus.
We adapt the description from~\cite[Chapter 17]{podc}.
%The Byzantine consensus problem is a classical problem is distributed systems literature.
%It is first described by Leslie Lamport as the Byzantine general's problem in 1982~\cite{lamport1982byzantine}.
%Byzantine consensus is described as follows (adapted from ).
\begin{definition}
\textbf{\emph{Byzantine Consensus}}

There are $n$ nodes, of which at most $t$ might experience Byzantine fault.
Node $i$ starts with an input value $v_i$.
The nodes must decide for one of  those values, satisfying the the following.
\begin{enumerate}
    \item \emph{Agreement:}
        If a correct node outputs $v$, then every node outputs $v$.
    \item \emph{Validity:}
        The decision value must be the input value of a node.
    \item \emph{Termination:}
        All correct nodes terminate in finite time.
\end{enumerate}
\end{definition}
A node under Byzantine fault means that it can have arbitrary behaviour.
For example not sending message or colluding with other Byzantine nodes to undermine the entire system.
Note that the decision is on a single value.
This is in contrast to ACS which we describe next.

ACS shares many similarities with Byzantine consensus.
But it is an especially useful primitive for blockchain systems.
It allows any party to propose a value and the result is the set union of all the proposed values by the majority.
This is the primary difference with Byzantine consensus.
Concretely, ACS needs to satisfy the following properties (adapted from~\cite{miller2016honey}).
\begin{definition}
\label{def:acs}
\textbf{\emph{Asynchronous Subset Consensus}}

There are $n$ nodes, of which at most $t$ might experience Byzantine fault.
Node $i$ starts with an non-empty set of input values $C_i$.
The nodes must decide an output $C$, satisfying the following.
\begin{enumerate}
    \item \emph{Agreement:}
        If a correct node outputs $C$, then every node outputs $C$.
    \item \emph{Validity:}
        If any correct node outputs a set $C$,
        then $|C| \ge n - t$ and $C$ contains the input of at least $n - 2t$ nodes.
    \item \emph{Totality:}
        If $n - t$ nodes receive an input, then all correct nodes produce an output.
\end{enumerate}
\end{definition}

ACS has the nice property of censorship resiliance when compared to other consensus algorithms.
For instance, Hyperledger and Tendermint uses Practical Byzantine Fault Tolerance (PBFT)~\cite{castro1999practical} as their consensus algorithm.
In PBFT, a leader is elected, if the leader is malicious but follows the protocol, then it can selectivly filter transactions.
In contract, every party in ACS are involved in the proposal phase,
and it guarantees that if $n - 2t$ parties propose the same transaction, then it must be in the agreed output.
Thus, if some value is submitted to at least $n - 2t$ nodes, it is guaranteed to be in the consensus result.
For a detailed description of ACS we refer to the HoneyBadgerBFT work~\cite{miller2016honey}.

The main drawback with ACS and also Byzantine consensus algorithms is the high message complexity.
Typically, such protocols have a message complexity of $O(n^2)$, where $n$ is the number of parties.
Hence, it may work with a small number of nodes,
but it is infeasible for blockchain systems where thousands of nodes are involved.

% The literature on Byzantine consensus and atomic broadcast is rich, some noteable ones include TODO.
% Thus in the ETC consensus protocol, we assume there exist an "off-the-shelf" Byzantine consensus algorithm which we can use
% (in our implementation we use HoneyBadgerBFT and motivate our choice in TODO).

% In our case, every node do not propose some arbitrary data, but a set of CP blocks.
% Thus the result of the consensus is the set union of all the legitimate proposals.


\subsection{Bootstrap phase}
\label{sec:bootstrap}
Now we have all the necessary information to describe our consensus protocol.
We begin with the bootstrap phase and then move onto the actual consensus phase.

Recall that facilitators are computed from the consensus result,
but the consensus result is agreed by the facilitators.
Thus we have a dependency cycle.
The goal of the bootstrap phase is to give us a starting point in the cycle.

To bootstrap, imagine that there is some bootstrap oracle, that initiates the code on every node.
The code satisfied all the properties in \Cref{def:consensus}.
Namely every node has the same set of valid facilitators $\F_1$ that are randomly choosen.
This concludes the bootstrap phase.

In practice, the bootstrap oracle is most likely the software developer and some of the desired properties cannot be achieved.
In particular, it is not possible to have the fairness property because it is unlikely that the developer knows the identity of every node in advance.

\subsection{Consensus phase}
\label{sec:consensus-phase}
The consensus phase begins when $\F_r$ is available to all the nodes.
Note that $\F_r$ indicates the facilitators that were elected using result of round $r$ and are responsible for driving the ACS protocol in round $r + 1$.
The goal is to reach agreement on a set of new facilitators $\F_{r + 1}$ that satisfies the four properties in \Cref{def:consensus}.

There are two scenarios in the consensus phase.
First, if node $u$ is not the facilitator, it sends $\langle \texttt{cp\_msg}, c_{u, h} \rangle$ to all the facilitators.
Second, if the node is a facilitator, it waits until it has received $N - t$ messages of type \texttt{cp\_msg}.
% Second if the node is a facilitator, it waits for some duration $D$ and collect messages of type \texttt{cp\_msg}, where $D \gg \Delta$.
Invalid messages are removed, namely blocks with invalid signatures and duplicate blocks signed by the same key.
With the sufficient number of \texttt{cp\_msg} messages,
it begins the ACS algorithm and some $\C'_{r + 1}$ should be agreed upon by the end of it.
Duplicates and blocks with invalid signatures are again removed from $\C'_{r+1}$ and we call the final result $\C_{r+1}$
We have the remove invalid blocks a second time (after ACS) because the adversary may send different CP blocks to different facilitators,
which results in invalid blocks in the ACS output.

At the core of the consensus phase is the ACS protocol.
While any ACS protocol that satisfies the standard definition will work,
we use a simplification of HoneyBadgerBFT~\cite{miller2016honey} as our ACS protocol
because it is the only (to the best of our knowledge) consensus algorithms designed for blockchain systems.
We do not use the full HoneyBadgerBFT due to the following.
First, the transactions in HoneyBadgerBFT are first queued in a buffer and the main consensus algorithm starts only when the buffer reaches an optimal size.
We do not have an infinite stream of CP blocks, thus buffering is unsuitable.
Second, HoneyBadgerBFT uses threshold encryption to hide the content of the transactions.
But we do not reach consensus on transactions, only CP blocks, so hiding CP blocks is meaningless for us as it contains no transactional information.

Continuing, when $\F_{r}$ reaches agreement on $\C_{r+1}$,
they immediately broadcast two messages to all the nodes---
first the consensus message $\langle \texttt{cons\_msg}, \C_{r+1} \rangle$,
and second the signature message $\langle \texttt{cons\_sig}, r, sig \rangle$.
The reason for sending \texttt{cons\_sig} is the following.
Recall that channels are not authenticated, 
and there are no signatures in $\C_{r + 1}$.
If a non-facilitator sees some $\C_{r + 1}$, it cannot immediately trust it because it may have been forged.
Thus, To guarantee authenticity, every facilitator sends an additional message that is the signature of $\C_{r + 1}$.

Upon receiving $\C_{r + 1}$ and at least $n - f$ valid signatures by some node $u$, $u$ performs two asks.
First, it creates a new CP block using $\textsf{new\_cp}(\C_{r + 1}, r + 1)$ (\Cref{alg:new-cp}).
Second, it computes the new facilitators using $\textsf{get\_facilitator}(C, n)$ (\Cref{alg:facilitator})
and updates its facilitator list to the result.
This concludes the consensus phase and brings us back to the beginning of the consensus phase.

\begin{algorithm}
\caption{Function $\textsf{get\_facilitator}(C, n)$ takes a list of CP blocks $C$ and an integer $n$,
sort evey element in $C$ by its luck value (the $\lambda$-expression), and outputs the smallest $n$ elements.}
\label{alg:facilitator}
\begin{algorithmic}
\State $\textsf{take} (n, \textsf{sort\_by} (\lambda x.\textsf{H}(x || pk \text{ of } x), C))$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Function $\textsf{new\_cp}(\C_r, r)$ runs in the context of the caller $u$.
It creates a new CP block and appends it to $u$'s chain.}
\label{alg:new-cp}
\begin{algorithmic}
\State $h \gets |B_u|$
\State $c_{u, h} \gets \langle \textsf{H}(b_{u, h-1}), h, \textsf{H}(\C_r), r, sig_u \rangle$
\State $B_u \gets B_u \cup c_{u, h}$
\end{algorithmic}
\end{algorithm}

Our protocol has some similarities with synchronisers~\cite[Chapter 10]{podc} because it is effectively a technique to introduce synchrony in an asynchronous environment.
If we consider the facilitators is a collective authority, then it can be seen as a synchroniser that sends pulse messages (in the form of \texttt{cons\_msg} and \texttt{cons\_sig}) to indicate the start of a new clock pulse.
Every node then sends a completion messages (in the form of \texttt{cp\_msg}) to the collective authority to indicate that they are ready to start a new pulse.


\section{Transaction protocol}
\label{sec:tx-protocol}

The TX protocol, shown in \Cref{alg:tx-proto}, is run by all nodes.
It is also known as True Halves, first described by Veldhuisen~\cite[Chapter~3.2]{truehalves}.
Node that wish to initiate a transaction calls $\textsf{new\_tx}(pk_v, m, txid)$ (\Cref{alg:new-tx}) with the intended counterparty $v$ identified by $pk_v$ and message $m$.
$txid$ should be a uniformly distributed random value, i.e. $txid \in_R \{0, 1\}^{256}$.
Then the initiator sends $\langle \texttt{tx\_req}, t_{u, h}\rangle$ to $v$.

\begin{algorithm}
    \caption{Function $\textsf{new\_tx}(pk_v, m, txid)$ generates a new TX block and appends it to the caller $u$'s chain.
    It is executed in the private context of $u$, i.e. it has access to the $sk_u$ and $B_u$.
    The necessary arguments are the public key of the counterparty $pk_v$, the transaction message $m$ and the transaction identifier $txid$.}
    \label{alg:new-tx}

    \begin{algorithmic}
    \State $h \gets |B_u|$
    \State $t_{u, h} \gets \langle \textsf{H}(b_{u, h - 1}), h, txid, pk_v, m, sig_u \rangle$
    \State $B_u \gets B_u \cup \{ t_{u, h} \}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{The TX protocol which runs in the context of node $u$.}
    \label{alg:tx-proto}

    \begin{algorithmic}
        \Upon $\langle \texttt{tx\_req}, t_{v, j} \rangle$ from $v$
        \State $\langle \_, \_, txid, pk_v, m, \_ \rangle \gets t_{v, j}$
        \State $\textsf{new\_tx}(pk_u, m, txid)$
        \State store $t_{v, j}$ as the pair of $t_{u, h}$
        \State send $\langle \texttt{tx\_resp}, t_{u, h} \rangle$ to $v$

        \Upon $\langle \texttt{tx\_resp}, t_{v, j} \rangle$ from $v$
        \State $\langle \_, \_, txid, pk_v, m, \_ \rangle \gets t_{v, j}$
        \State store $t_{v, j}$ as the pair of the TX with identifier $txid$
    \end{algorithmic}
\end{algorithm}

A key feature of the TX protocol is that it is non-blocking.
At no time in \Cref{alg:new-tx} or \Cref{alg:tx-proto} do we need to hold the chain state and wait for some message to be delivered before committing a new block to the chain.
This allows for high concurrency where we can call $\textsf{new\_tx}(\cdot)$ multiple times without waiting for the corresponding \texttt{tx\_resp} messages.

\section{Validation protocol}
\label{sec:vd-protocol}

Up to this point, we do not provide a mechanism to detect forks or other forms of tampering or forging.
The validation protocol aims to solve this issue.
The protocol is also a request-response protocol, just like the transaction protocol.
But before explaining the protocol itself, we first define what it means for a transaction to be valid.

\subsection{Validity definition}
A transaction can be in one of three states in terms of validity---\emph{valid}, \emph{invalid} and \emph{unknown}.
Given a fragment $F_{v, j}$, the validity of the transaction $t_{u, i}$ is captured by the function $\textsf{get\_validity}(t, F)$ (\Cref{alg:get-validity}).
The first four conditions (up to \Cref{line:valid-fragment}) essentially check whether the fragment is the one that the verifier needs.
If it is not, then the verifier cannot make any decision and return \emph{unknown}.
This is likely to be the case for new transactions because $\textsf{agreed\_fragment}(\cdot)$ would be $\bot$.
The next two conditions checks for tampering or missing blocks, if any of these misconducts are detected, then the TX is invalid.

Note that the validity is on a transaction (two TX blocks with the same $txid$), rather than on one TX block owned by a single party.
It is defined this way because the malicious sender may create new TX blocks in their own chain but never send \texttt{tx\_req} messages.
In that case, it may seem that the counterparty, who is honest, purposefully ommitted TX blocks.
But in reality, it was the malicious sender who did not follow the protocol.
Thus, in such cases the whole transaction, identified by its $txid$ is marked as invalid.

Further, the caller of $\textsf{get\_validity}(t_u{u, i}, F_{v, i})$ is not necessarily 
$u$\footnote{In practice it often is because after completing the TX protocol the parties are incentivised to check that the counterparty ``did the right thing''.}
Any node $w$ may call $\textsf{get\_validity}(t_u{u, i}, F_{v, i})$ as long as the caller $w$ has an agreed fragment of $t_{u, i}$---$F_{u, i}$.
$F_{u, i}$ may be readily available if $w = u$ or it may be from some other \texttt{vd\_resp} message, which we describe next in the validation protocol.

\begin{algorithm}
\caption{Function $\textsf{get\_validity}(t_{u, i}, F_{v, j})$ validates the transaction $t_{u, i}$.
$F_{v, j}$ is the corresponding fragment received from $v$.}
We assume there exist a valid $F_{u, i}$, namely the agreed fragment of $t_{u, i}$. 
The caller is $w$, it may be $u$ but this is not necessary.
\label{alg:get-validity}

\begin{algorithmic}[1]

    % \State $F_{u, i} \gets \textsf{agreed\_fragment}(t_{u, i})$
    % \If{$F_{u, i} = \bot$}
    %     \State \Return \emph{unknown}
    % \EndIf \Comment $u$ has agreed fragment
    % \State 

    \State $c_{v, a} \gets \textsf{first}(F_{v, j})$
    \State $c_{v, b} \gets \textsf{last}(F_{v, j})$
    \If{$c_{v, a}$ or $c_{v, b}$ are not in consensus}
        \State \Return \emph{unknown}
    \EndIf
    \State \Comment $v$ has agreed fragment

    % \If{$|F_{v, j}| > L$}
    %     \State \Return \emph{unknown}
    % \EndIf \Comment fragment not too long
    % \State

    % txid are equal now, but bad hash pointer
    \If{sequence number in $F_{v, j}$ is correct (sequential)}
        \If{hash pointers in $F_{v, j}$ is wrong}
            \State \Return \emph{unknown}
        \EndIf
    \EndIf
    \State \Comment correct TrustChain structure

    \State $c_{u, b} \gets \textsf{last}(F_{u, i})$
    \If{$c_{u, b}$ is not created using the same $\C_r$ as $c_{v, b}$}
        \State \Return \emph{unknown}
    \EndIf
    \State \Comment correct consensus round
    \label{line:valid-fragment}
    \State $\langle \_, \_, txid, pk_v, m, \_ \rangle \gets t_{u, i}$
    \If{number of blocks of $txid$ in $F_{v, j} \ne 1$}
        \State \Return \emph{invalid}
    \EndIf
    \State \Comment TX exists 

    \State $\langle \_, \_, txid', pk'_u, m', \_ \rangle \gets t_{v, j}$
    \If{$m \ne m' \vee pk_u \ne pk'_u$}
        \State \Return \emph{invalid}
    \EndIf
    \State \Comment no tampering
    \State \Return \emph{valid}
\end{algorithmic}
\end{algorithm}

\subsection{Validation protocol}
With the validity definition, we are ready to construct a protocol for determining the validity of transactions.
The protocol is a simple response and request protocol (\Cref{alg:vd-proto}).
If $u$ wishes to validate some TX with ID $txid$ and counterparty $v$, it sends $\langle \texttt{vd\_req}, txid \rangle$ to $v$.
The desired properties of the validation protocol are as follows.
\begin{definition}
\label{def:validation}
\textbf{\emph{Properties of the validation protocol}}

\begin{enumerate}
    \item \emph{Correctness}:
        The validation protocol outputs the correct result
        according to the aforementioned validity definition.
    \item \emph{Agreement}:
        If any correct node decides on the validity (except when it is \emph{unknown}) of a transaction,
        then all other correct nodes are able to reach the same conclusion or \emph{unknown}.
    % \item \emph{Unforgeability}:
    %     If some transaction is valid, it cannot be forged into an invalid transaction.
    %     If some transaction is invalid, it cannot be forged into a valid transaction.
    \item \emph{Liveness}:
        Any valid transactions can be validated eventually.
\end{enumerate}
\end{definition}

\begin{algorithm}
\caption{Validation protocol}
\label{alg:vd-proto}

\begin{algorithmic}
    \Upon $\langle \texttt{vd\_req}, txid \rangle$ from $v$
        \State $t_{u, i} \gets \text{the transaction identified by } txid$
        \State $F_{u, i} \gets \textsf{agreed\_fragment}(t_{u, i})$
        \State send $\langle \texttt{vd\_resp}, txid, F_{u, i} \rangle$ to $v$

    \Upon $\langle \texttt{vd\_resp}, txid, F_{v, j} \rangle$ from $v$
        \State $t_{u, i} \gets \text{the transaction identified by } txid$
        \State set the validity of $t_{u, i}$ to $\textsf{get\_validity}(t_{u, i}, F_{v, j})$
\end{algorithmic}
\end{algorithm}

We make two remarks.
First, just like the TX protocol, we do not block at any part of the protocol.
Second, suppose some $F_{v, j}$ validates $t_{u, i}$, then that does not imply that $t_{u, i}$ only has one pair $t_{v, j}$.
Our validity requirement only requires that there is only one $t_{v, j}$ in the correct consensus round.
The counterparty may create any number of fake pairs in a later consensus rounds.
But these fake pairs only pollutes the chain of $v$ and can never be validated because the round is incorrect.

\section{Design variations and tradeoffs}
\label{sec:tradeoffs}

Up to this point,
we have discussed our protocol in the context of the model and assumptions defined in \Cref{sec:model-assumptions}.
In this section, 
we explore a few design variations which we can make, some of them require a relaxed version of our original model.
They enable better performance, allow us to apply our design in the fully permissionless setting and improves privacy.

\subsection{Using epidemic protocol to reduce communication cost}
One of the final steps in our consensus protocol is to broadcast the consensus result and signatures to every other node (\Cref{sec:consensus-phase}).
While this guarantees delivery (since we assumed reliable channel), it is wasteful.
For example, if every facilitator is honest, a node would receive $n$ consensus results which are identical when only one is necessary.

An optimisation we can do is to use an epidemic protocol~\cite{eugster2004epidemic} (also known as gossiping)) instead of our broadcast approach.
Typical epidemic protocols works as follows.
Every node buffers every message it receives up to some buffer size $b$.
Then it forwards the messages $t$ number of times.
Every time the message is sent to $f$ random neighbours, $f$ is often called the fan-out.
The upside of using epidemic protocol is that the communication cost is distributed more evenly between nodes.
This is especially true with a lazy push approach the node who just received a message would push the message ID to its $f$ random neighbours,
and only push the full message if the neighbour explicitly requests for the message~\cite{leitao2007epidemic}.
With this, nodes typically only need to receive one consensus result message instead of $n$.

A down side of epidemic protocol is that it usually takes $O(\log N)$ time to infect the whole network,
whereas broadcasting uses constant time.
Another downside of some epidemic protocols (e.g. eager push) is that it is difficult to guarantee delivery.
It is especially true when the parameters are not choosen correctly in a network that is only partially connected (but every node is nevertheless reachable).
If the delivery cannot be guaranteed, 
then we cannot guarantee liveness in our consensus protocol because a future facilitator may miss the memo.
Picking parameters are difficult in practice because the network configuration is unknown and the total number of nodes might also be unknown.


\subsection{Using timing assumption in the permissionless setting}
Our model is purely asynchronous, where we make no timing assumptions anywhere in the protocol.
In many applications however, it is often fine to make timing assumptions.
For example, TCP relies on timeout for its retransmission and the nLockTime property in Bitcoin transactions makes the transaction unspendable until some time in the future (either Unix time or block height)~\cite{bitcoindevguide}.
One limitation of our system is that we use the parameter $N$ in our algorithms, which makes it unsuitable for the permissionless environment where users can join and leave at will.
In this section we show how making a timing assumption would allow us to operate in the permissionless setting.

At the start of our consensus phase (\Cref{sec:consensus-phase}), facilitators must wait for $N-f$ \texttt{cp\_msg} messages.
This is the only place where we used $N$ as a parameter.
To introduce timing, instead of waiting for $N-f$ messages, we wait for some time $D$,
such that $D$ is sufficiently long for honest nodes to send their CP blocks to the facilitators.
Again, choosing the parameter $D$ is difficult and depends on a number of factors such as the network condition, message size, and so on.
Overestimating it would make agreed fragments much longer than usual, which increases communication costs for validation.
Underestimating it would lead to unfairness where users that are too late do not have a chance to be selected as a facilitator in the next round.
Nevertheless, there is a significant gain for making the timing assumption and that is the ability to operate in the permissionless setting which we explain next.

Suppose a new node wish to join the network and the facilitators are known (this can be done with a public registry).
It simply sends its latest CP block to the facilitators.
Then, in the next round the node will have a chance to become a facilitator just like any existing node.
To leave the network, nodes simply stop submitting CP blocks.
There is a subtlety here which happens when the node is elected as a facilitator in the following round.
In this case, the node must fulfill its oblication by completing the consensus protocol (but without proposing its own CP block) before leaving.
Otherwise the $n \ge 3t + 1$ condition may be violated.

\subsection{Privacy preserving validation protocol using compact blocks}
\label{sec:compact}
Our approach already has privacy preserving features in comparison to early blockchain systems.
That is, the transactions for each node are only revealed during the validation protocol.
Hence if two nodes never directly or indirectly interact with each other,
their transactions are never revealed.

We can take our privacy-preserving property one step further by introducing another level of hash pointer indirection.
The result is shown in~\Cref{fig:compact}.
\begin{figure}
    \includegraphics[width=0.7\textwidth]{compact}
    \centering
    \caption{The chain on the left represent direct chaining, where the digest in ``Prev'' is simply the digest of the previous block.
    The chain on the right uses compact blocks, represented by the smaller squares.
    Compact blocks also form a chain as before, but they each have a hash pointer to the full block, identified by ``Seq'' or ``Digest''.}
    \label{fig:compact}
\end{figure}

Concretely, we introduce an additional block type,
namely compact block.
Such blocks only have three attributes,
\begin{enumerate}
\item Seq---the sequence number its corresponding block,
\item Digest---the digest of its corresponding block, and
\item Prev---the digest of the previous compact block.
\end{enumerate}
Each compact block has a corresponding full block (either a CP block or a TX block).
The relationship is uniquely identified with Seq or Digest.
Recall that our original validation protocol requires the nodes to send the full agreed fragment.
With compact blocks, it is only necessary to send the compact version of the agreed fragment.
The validation then proceeds in a similar fashion,
provided that the pair of the to-be-validated TX block is known.

The space saving of this approach of course depends on the size of the full blocks.
If the full blocks are on average 500 bytes
(which is the typical size of Bitcoin transactions ~\cite{txsize}),
and the compact blocks are $32 + 32 + 8 = 72$ bytes
(SHA256 digests are 32 bytes each and we use a 64 bit integer to represent the sequence number),
then we have a 86\% saving in communication cost.

%Using compact blocks, it is possible to reduce network communication costs in the validation protocol but also protect transaction privacy.

\subsection{Optimising validation protocol using cached agreed fragments}
\label{sec:caching}
One more way to improve the efficiency of the validation protocol is to use a single agreed fragment to validate multiple transaction.
Concretely, upon receiving an agreed fragment from node $A$,
rather than validating a single transaction,
we attempt to validate all transactions in the unknown state performed with $A$ in that fragment.

For a node, the benefit of this technique is maximised when it only transacts with one other node.
In this case, the communication of one fragment is sufficient to validate all transactions in that fragment.
In the opposite extreme, if every transaction that the node makes is with another unique node,
then the caching mechanism would have no effect.

\subsection{Global fork detection}
The validation algorithm guarantees that there are no forks within a single agreed fragment.
This is sufficient for some applications such as proving the existance of some information.
However, applications such as digital cash where every block depends on some previous block,
then our scheme is not suitable.

There are a variety of approaches to do global fork detection.
First and the easiest solution is to simply ask for every agreed fragment from the one containing the genesis block up to the one that the to-be-proven TX block is in.
If the hash pointers are correct and all the CP blocks are in consensus, then the verifier can be sure that there are no forks.
We use this approach in our prior work on Implicit Consensus~\cite{implicitconsensus}.
It may sound inefficient at first, but nodes can employ caching to minimise communication costs.
In our work we call this ``spontaneous sharding''.

The second approach is probabilistic but with only a constant communication overhead.
For a node, observe that if every of its agreed fragment has a transaction with a honest node,
then the complete chain is effectively validated in a distributed maner.
The only way for an attacker to make a fork is to make sure that the agreed fragment containing the fork has no transactions with honest nodes.
We claim such malicious behaviour is prevented probabilisticaly using a challenge-response protocol as follows.
Suppose node $A$ wish to make a transaction with node $B$.
$A$ first sends a challenge to $B$ asking it to proof that it holds a valid agreed fragment between some consensus round specified by $A$.
If $B$ provides a correct proof, then they run the transaction protocol as usual.
If $B$ provides an invalid proof or refuses to respond,
then $A$ would refuse to make the transaction.
The probability that an honest node catches out a malicious node is 
$$p = \frac{f}{r},$$
where $f$ is the number of bad agreed fragments and $r$ is the latest round number.
If there are more nodes (say $n$) trying to make transactions with the malicious node,
then the probability that the malicious node does gets caught at least once follows a binomial distribution
$$\Pr[X > 0] = \sum_{k = 1}^{n}\binom{n}{k}p^k (1 - p)^{n - k}.$$

% subtlety when fork happens on CP block


% Another assumption we made in this work is the existance of a PKI.
% This can also be removed with the introduction of timing by storing the public keys the consensus results,
% much like in Bitcoin.


% \section{Universally Composable Framework}
% \label{sec:uc-intro}

% In order to analyse the security of a system, a formal notion of security is required.
% For instance, what does it mean if an encryption algorithm is secure?
% One may say it is secure if the adversay cannot learn any information about the plaintext from the ciphertext.
% But what if the adversay has some background information, for instance she may know it is English.
% Do we then still say the encryption algorithm is secure?
% Goldwasser and Micali introduced the notion of semantic security~\cite{goldwasser1982probabilistic}.
% That is, imagine two worlds, a real world and an ideal world.
% In the real world, the adversay is given the ciphertext,
% and in the ideal world the adversay has nothing.
% Then the encryption algorithm is semantically secure if the amount of information that the adversay can learn in the real world is just as much as the ideal world.
% While our description is informal, the notion of security can be formally captured in this way.

% Security sensitive distributed systems such as secure multi-party computation and blockchain systems also require a formal notion of security so that they can be analysed.
% Fortunately, the idea from semantic security can also be applied in a distributed setting.
% In an ideal world, we create an ideal functionality $\Fideal$ that performs all the computation on behalf of every node in the network.
% The nodes act as dummies and only relay messages between $\Fideal$ and the environment.
% Thus $\Fideal$ essentially becomes the specification of the distributed protocol.
% If the execution of the real world protocol is indistinguishable from the ideal emulation in the presence of some adversay,
% then the real world protocol is secure as per the ideal specification.
% This is in essence the Universally Composable (UC) framework.

% We use the UC framework not only because it suits our needs, 
% it is also the only framework used in modelling blockchain systems to the best of our knowledge
% ~\cite{todo}TODO.
% In the remainder of this section we give an overview of the UC framework.
% A detailed treatment  can be found in~\cite{canetti2001universally}.

% \subsection{Model of Computation}
% The model of computation considered in the UC framework is the interactive Turing machine (ITM).
% Specifically, an ITM is an extension of the Turing machine with externally writable tapes which are the following.
% \begin{enumerate}
% \item input tape---TODO
% \item incoming communication tape---TODO
% \item subroutine output tape---TODO
% \end{enumerate}
% ITM can be seen as a specification or an algorithm, a machine running an ITM is an ITM instance (ITI).
% To communicate, an ITI can write on the externally writable tapes of other ITIs.
% The writing ITI then pauses exeuction, the receiving ITI begins execution.
% Consequently, only one ITI is running at any point in time.

% \subsection{Simulation-based Security}
% Simulation-based security, also known as ideal/real paradigm is a model for defining security.
% The model consist of a set of , the environment $\E$, the adversay $\A$,
% the protocol ITM $\Preal$ and zero or more ideal functionalities $\mathcal{F}_0, \mathcal{F}_1, \dots$.
% The 
% Other than the machines running the protocol under consideration, the model contains two extra entities,
% the environment $\E$ and the adversary $\A$.
% $\E$ can be seen as users of the protocol, it can only provide input and receive output from $\A$ and the protocol.

% Control function TODO

% \subsection{Universal Composability}

% \section{Formal Specification}
% \label{sec:formal-model}

% The formal model follows the same structure as \ref{fig:todo}TODO.

% \begin{figure}[h]
% \begin{framed}
% \small  % end with \normalsize

% \textbf{Protocol}\,$\Preal$

% The ETC protocol. On initialisation do the following.
% \begin{itemize}
% \item Generate public and private key pair, $pk$ and $sk$.
% \item Set personal chain $C := \{genesis()\}$.
% \item Set checkpoint buffer $\C' := \varnothing$.
% \item Set the facilitators $F$ to the bootstrap nodes provided by $\E$.
% \item Set the latest round $r_{\text{latest}} = 0$.
% \end{itemize}

% Run the protocol as specified below after initialisation.
% \begin{itemize}

% \item Upon (\texttt{consensus}, $D$) from $\F^r_\text{BFT}$,
%     if $r_{\text{latest}} >= r$ then do nothing and pause,
%     otherwise do $try_add_cp(D, r)$
%     send (\texttt{propose})

% \item Upon (\texttt{consensus}, $D$, $r$) from $p \in \P$,

% \item Upon (\texttt{checkpoint}, $c$) from $p \in \P$,

% \item Upon (\texttt{tx\_init}, $m$, $p$) from $\E$, 
%     $C := C \cup \{new\_tx(m)\}$,
%     send (\texttt{tx\_req}, latext TX in $C$) to $p$.

% \item Upon (\texttt{tx\_req}, $m$) from $p \in \P$,
%     $C := C \cup \{new\_tx(m)\}$,
%     let $m'$ be the latest TX in $C$ and set $m$ to be the other half of $m'$,
%     send (\texttt{tx\_resp}, $m'$) to $p$.
%     Output the complete TX to $\E$.

% \item Upon (\texttt{tx\_resp}, $m'$) from $p \in \P$,
%     find the corresponding pair of $m'$ and name it $m$,
%     add $m'$ as the other half of $m$.
%     Output the complete TX to $\E$.

% \item Upon (\texttt{vd\_init}, $s$) from $\E$,
%     if the sequence number $s$ does not exist in $C$ or the block with $s$ does not have the other half then do nothing.
%     Otherwise send (\texttt{vd\_req}, $s'$) to $p'$ where $s'$ is the sequence number of the other half and $p'$ is the counterparty.

% \item Upon (\texttt{vd\_req}, $s'$) from $p \in \P$,
%     if the sequence number $s'$ does not exist then do nothing.
%     Otherwise send (\texttt{vd\_resp}, $agreed\_fragment(s')$) to $p$.

% \item Upon (\texttt{vd\_resp}, $x$) from $p \in \P$,
%     run $validate(x)$ and output result to $\E$.

% \end{itemize}

% \normalsize
% \end{framed}
% \end{figure}

% \begin{framed}
% \textbf{Functionality}\,$\F^r_\text{BFT}$

% The ideal Byzantine consensus protocol, parameterised by the round number $r$.

% \begin{itemize}
% \item Upon (\texttt{propose}, $r$, $C$),
% \item Upon (\texttt{fetch}),
% \end{itemize}
% \end{framed}

% \subsection{Discussion}

% \subsubsection{Global clock for synchronisation}
% Where there is no Byzantine corruption, we conjecture that our protocol runs in the asynchronous setting.
% However, security of many Byzantine consensus algorithms, especially the one we adopted---HoneyBadgerBFT,
% fall apart when there is dynamic corruption.
% In order to enforce that the corrupted machines do not change when running an instance of some Byzantine consensus algorithm,
% we introduce synchrony.

% We make use of a global clock... TODO

% Dynamic corruption only between different clock ticks is enforced by the control function.

% \subsubsection{Static corruption versus dynamic corruption}
% A subtlety exists when modelling the type of corruption.
% In dynamic corruption\footnote{In \cite{canetti2001universally}, dynamic corruption is termed Byzantine corruption.},
% $\A$ take full control of a number of machines and also learns all of their states.
% The states include private keys.
% Thus, using the dynamic corruption model from \cite{canetti2001universally} we cannot guarantee security as
% $\A$ can corrupt different machines over time and eventually learn all the private keys..

% First is to use static corruption, where the corrupted machines are fixed.
% While this circumvents our problem, it is a much weaker model.
% Alternatively, we modify the aforementioned dynamic corruption model and weaken the adversary's abilities.
% In particular, we introduce that forgetful adversary that only remembers the state of the currently corrupted nodes and forget the state of the recovered nodes.


