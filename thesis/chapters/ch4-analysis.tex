\chapter{Analysis of correctness and performance}
\label{ch:analysis}

In this chapter,
we evaluate our system analytically to check whether it correctly implements the TrustChain consensus protocol (\Cref{def:consensus})
and the TrustChain validation protocol (\Cref{def:validation}).
Further, we analyse the performance and in particular the throughput to understand the scalability characteristics.
Finally, we consider the case where the $n \ge 3t + 1$ assumption is violated and show the effect of it in our system.

\section{Correctness analysis}
\label{sec:correctness}
Our first objective is to show that our consensus protocol $\Pi_c$ described in~\Cref{sec:cons-protocol} is,
in fact, a TrustChain consensus protocol defined in~\Cref{def:consensus}.
Then, building on top of it, we show that our validation protocol $\Pi_v$ described in~\Cref{sec:vd-protocol} conforms to the TrustChain validation protocol defined in~\Cref{def:validation}.
The resulting theorem shows that only using CP blocks in the consensus algorithm implies consensus on TX blocks.

% Our first objective in this section is to establish truths regarding the correctness of our protocol.
% We do this in two parts.
% First we use mathematical induction to show that properties in \Cref{def:consensus} holds for all round.
% Building on top that, if \Cref{def:consensus} is true, then we can show that many properties in \Cref{def:validation} is true.

\subsection{Correctness of the consensus protocol}

We begin our analysis by establishing the four properties in \Cref{def:consensus},
namely agreement, validity, fairness and termination for an arbitrary round.
Using these results, we use mathematical induction to show that they hold for all rounds.

\begin{lemma}
\label{lemma:agreement}
If for an arbitrary round $r$,
$\F_{r}$ is known by all correct nodes and one correct node outputs a list of facilitators $\F_{r+1}$,
then all correct nodes output $\F_{r+1}$.
\end{lemma}
\begin{proof}
The argument follows from the protocol description.
Given that $\F_r$ is known,
correct nodes will send CP blocks to all members of $\F_r$.
For each facilitator, the ACS algorithm starts independently whenever the facilitator has $N - t$ valid CP blocks
(recall from \Cref{sec:consensus-phase} that invalid blocks are ones with an invalid signature or has a duplicate signature).
No honest facilitator can make progress until $n-t$ honest facilitators start algorithm,
but this eventually happens because there are $N - t$ correct nodes and all correct facilitator eventually receive $N -t$ valid CP blocks.
At the end of ACS, $\C_{r+1}$ is created and then broadcasted along with the signature of the facilitators.
Due to the agreement property of ACS (\Cref{def:acs}),
every correct node should receive at least $n - t$ valid signatures on the agreed $\C_{r+1}$.
Thus they use $\C_{r+1}$ to generate a new CP block and compute new facilitators.
Since $\textsf{get\_facilitators}(\cdot)$ is a deterministic algorithm and the input $\C_{r+1}$ is in agreement, the output $\F_{r+1}$ is also in agreement.
\end{proof}

\begin{lemma}
\label{lemma:validity}
If for an arbitrary round $r$,
$\F_r$ is known by all correct nodes and any correct node outputs $\F_{r+1}$,
then (a) $|\C_{r+1}| \ge N - t$ and (b) $|\F_{r+1}| = n$.
\end{lemma}
\begin{proof}
The validity follows from the validity property of ACS and the definition of our model,
namely $N \ge n + t$ and $n \ge 3t + 1$.
Given $\F_r$, since $N \ge n + t$ , there is at least $n$ nodes that would send their CP block to $\F_r$.
From the validity property of ACS, we know the output must contain the input of at least $n - 2t$ nodes.
But $n -t$ facilitators must have received $N - t$ valid CP blocks, so $|\C_{r+1}| \ge N-t$ and this proves (a).
Since $N-t \ge (n+t) -t = n$ and $\textsf{get\_facilitators}(\C_{r+1})$ outputs $n$ items, so $|\F_{r+1}| = n$ and this proves (b).
\end{proof}

\begin{lemma}
\label{lemma:fairness}
If for an arbitrary round $r$,
$\F_r$ is known by all correct nodes then
every node with a CP block in $\C_{r+1}$ should have an equal probability to be elected as a facilitator in $\F_{r+1}$.
\end{lemma}
\begin{proof}
We already established that $|\C_{r+1}| \ge N - t \ge n$ from \Cref{lemma:validity}.
Then the proof directly follows from the random oracle model.
Recall that the luck value is computed using $\textsf{H}(\C_{r+1} || pk_u)$.
Since $pk_u$ is unique for every node that has a CP block in $\C_{r+1}$,
the output of $\textsf{H}(\cdot)$ is uniformly random.
As a result, the output of $\textsf{get\_facilitators}(\C_{r+1})$ is a random permutation of all the nodes in $\C_{r+1}$ truncated to $n$.
Thus, every node has the same probability of becoming a facilitator.
\end{proof}

\begin{lemma}
\label{lemma:termination}
If for an arbitrary round $r$,
$\F_r$ is known by all correct nodes then
every correct node eventually outputs some $\F_{r + 1}$.
\end{lemma}
\begin{proof}
This follows directly from the properties of the channel (eventual delivery)
and the termination property of ACS.
That is, $\F_r$ eventually receives all the CP blocks required to begin ACS,
and then ACS eventually terminates.
Finally, the results are eventually disseminated to all the nodes.
\end{proof}

From Lemmas \ref{lemma:agreement}, \ref{lemma:validity}, \ref{lemma:fairness} and \ref{lemma:termination},
we have shown that the four properties of \Cref{def:consensus} holds when assuming the existence of some $\F_r$.
Now we need to proof these properties under the universal quantifier on $r$.
We do this using mathematical induction.

\begin{theorem}
\label{theorem:consensus}
$\Pi_c$ implements a TrustChain consensus protocol (\Cref{def:consensus}).
\end{theorem}
\begin{proof}
We prove using mathematical induction.

In the base case ($r = 1$), agreement, validity fairness and termination follows directly from the bootstrap protocol.
Note that the result is $\F_1$,
which represents the set of facilitators agreed in round 1,
are responsible for driving the ACS protocol in round 2.

For the inductive step,
we assume that the four properties hold in round $r$ and prove that they also hold in round $r + 1$.
Using Lemmas \ref{lemma:agreement}, \ref{lemma:validity}, \ref{lemma:fairness} and \ref{lemma:termination},
it directly follows from modus ponens that these properties hold for $r + 1$.
Due to the principals of mathematical induction, these properties hold for all $r$.
\end{proof}

\subsection{Correctness of the validation protocol}
$\Pi_c$ ensures consensus on CP blocks and facilitators,
which makes it the building block for reaching consensus on transactions.
In this section, we build on top of \Cref{theorem:consensus} to show that our validation protocol $\Pi_v$ described in \Cref{sec:vd-protocol} has the agreement and validity properties of a TrustChain validation protocol defined in \Cref{def:validation}.

\begin{theorem}
\label{theorem:validation-agreement}
% (Agreement of the validation protocol)
$\Pi_v$ satisfies the agreement and validity properties of a TrustChain validation protocol (\Cref{def:validation}).
\end{theorem}
\begin{proof}
We proof the agreement property by contradiction.
Recall that $F_{u, i}$ and $t_{u, i}$ used in \Cref{alg:get-validity} are valid.
Without loss of generality, suppose for some transaction with TX block $t$,
node $u$ decides \emph{valid} but node $v$ decides \emph{invalid}.
Then there exists a fragment $F' = \{ \dots, t', c'\}$ which $u$ received that contains a valid pair of $t$---$t'$.
There also exists a fragment $F'' = \{ \dots, t'', c''\}$ which $v$ received that does not contain or contains an invalid pair---$t''$.
In both cases, the $\textsf{get\_validity}(\cdot)$ function must have reached \Cref{line:valid-fragment}.
Due to \Cref{theorem:consensus}, we have $c' = c''$, otherwise the result would be \emph{unknown}.
Since $c' (= c'') = \langle \textsf{H}(t'), \dots \rangle$ we must have $\textsf{H}(t') = \textsf{H}(t'')$ and $t' \ne t''$ (because $t''$ is invalid).
In other words, the sender of $F''$ must be able to create some $t''$ that has the same digest as $t'$.
But this is only possible if the adversary can compute the inverse of $H(\cdot)$ with non-negligible probability.
Thus we have a contradiction and this completes the agreement proof.

$\Pi_v$ directly uses \Cref{alg:get-validity}, which is also the validity definition,
so the validity property is also satisfied.
\end{proof}

\Cref{theorem:validation-agreement} shows that agreement on CP blocks would lead to agreement on TX blocks when the nodes are running the validation protocol.
Like in our prior work~\cite{implicitconsensus}, we call this behaviour implicit consensus.
One of the main advantages of our scheme over running a consensus algorithm on all the transactions is that 
the transaction throughput is no longer dependent on the consensus algorithm---ACS.
This enables horizontal scalability where adding new nodes would lead to higher global transaction rate.
In addition, a convenient consequence of \Cref{theorem:validation-agreement} is unforgeability.
That is, no adversary can create two chains,
$F = \{ \dots, t, c\}$ and $F' = \{ \dots, t', c\}$,
with correct hash pointers and the same end of chain $c$ with non-negligible probability.
% Another way to think about it is if a transaction can be forged, then it cannot be agreed.

A stronger version of the validity definition exists.
That is, if two honest nodes make a transaction,
then the transaction state is always valid in addition to our current validity definition.
Under our purely asynchronous model, we cannot guarantee this stronger version.
Since the adversary can delay any message for any amount of time,
it can make sure all \texttt{tx\_req} messages are delivered in a round later than the round which the message is sent.
Effectively, the pair would always be in different rounds and the validation protocol would not output \emph{valid}.
We believe in a relaxed model, i.e. a weakly synchronous model, a stronger validity definition is possible.

\subsection{Impossibility of liveness}
\Cref{theorem:validation-agreement} is a major result that allows significantly improved performance over traditional blockchain systems,
but it does not have all the properties of a typical Byzantine consensus problem.
Now we show a negative result, where the liveness property of \Cref{def:validation} cannot be attained in $\Pi_v$,
meaning that transactions made with adversaries cannot always be validated.

\begin{lemma}
There exists a valid transaction that cannot be eventually validated.
\end{lemma}
\begin{proof}
Suppose nodes $u$ and $v$ correctly performed the TX protocol to create a transaction.
Then when $u$ wants to validate it, it does so by sending \texttt{vd\_req} message to $v$.
$v$ can act maliciously and ignore all \texttt{vd\_req} message from $u$, and then the transaction can never be validated.
\end{proof}
Although this is a negative result, it does not put the adversary in an advantageous position.
If the adversary is observed to ignore validation requests, then the honest nodes may prefer not to transact with her in the future.
Thus, to stay relevant in the system, the adversary needs to comply with the protocol.

% \subsection{Chain structure}
% In this section we discuss how the adversary may tamper with the chain structure and its effects.
% The adversary can create invalid blocks, which are blocks that have an invalid hash pointer.
% The result is some chain with a broken link.

\section{Performance analysis}
\label{sec:performance-analysis}
This section aims to answer whether our system is horizontally scalable analytically.
We begin by looking at the communication and time complexity of the consensus protocol $\Pi_c$,
and then the bandwidth requirement for a single transaction.
We build on top of those results to analyse the global throughput.

\subsection{Communication complexity of the consensus protocol}
\label{sec:cons-complexity}
$\Pi_c$ can be seen as three parts,
so the communication complexity is the sum of these parts.
The first part is when every node sends their CP block to all the facilitators, which is $O(Nn)$ since there are $N$ nodes and $n$ facilitators.
Or simply $O(N)$ if we consider $n$ as a constant.

The second part is ACS.
The communication complexity of ACS is $O(n^2|v| + \lambda n^3 \log n)$~\cite{miller2016honey},
where $|v|$ is the size of the largest message and $\lambda$ is the security parameter (described in \Cref{sec:model-assumptions}).
In our system, we wish to understand the scalability property.
Thus we consider the complexity as a function of $N$ rather than $n$ or $\lambda$.
Since $|v|$ is at most all the CP blocks from every node, we have $|v| = kN$,
where $k$ is a constant representing the size of one CP block.
Therefore the communication complexity of ACS in our system is $O(N)$.
Since we use a constant $n$, $O(N)$ communication complexity also holds for a single facilitator.

% TODO wrong?
The third and final part is the dissemination, where the facilitators broadcast the consensus result along with their signatures.
For the same reason as the first part, this is also $O(N)$.
Thus the combined communication complexity is $O(N)$ for a constant $n$.

% We argue that with a $O(N)$ message complexity we also have $O(N)$ time complexity.
% It is know that the running time is $O(\log n)$~\cite{miller2016honey}, which translates to $O(1)$ if our $n$ is constant.
% On the other hand, in many distributed algorithms, computational cost is small when compared to the communication cost.
% But in our system, we are more interested in the effect of the message size.

\subsection{Duration of the consensus protocol}
\label{sec:cons-duration}
To make arguments on the duration of $\Pi_c$, the bandwidth or throughput in our purely asynchronous model,
which are concepts that depend on time,
we must make additional assumptions.
Note that the duration we are interested in is not the same as the time complexity typically used in distributed systems.
In the analysis of distributed algorithms, time complexity is often in terms of the number of rounds.
For example, ACS runs in a constant number of rounds because of its sub-protocols---reliable broadcast and binary Byzantine consensus---also run in a constant number of rounds~\cite{miller2016honey}.
However, in practice, making a unit of communication always has some overhead associated with it, for example serialising and writing it to some network socket.
Hence, for the remainder of our performance analysis, we add the following to our computational model.
For every unit of communication, we assume they take some non-negligible but constant time to perform.
Hence, from our results in~\Cref{sec:cons-complexity} and the fact that $\Pi_c$ runs in a constant number of rounds,
it follows that $\Pi_c$ has a duration of $O(N)$.

\subsection{Communication cost for transactions}
\label{sec:communication-cost-for-tx}
Geared with the results above,
we are ready to analyse the amount of data required to be transmitted over a link per transaction,
which we call the communication cost per transaction.
To create and then validate a transaction,
the communication cost per transaction is of $O(l)$,
where $l$ is the length of the agreed fragment.
This result can be seen from the fact that the largest message by far is the \texttt{vd\_resp} message,
which contains the agreed fragment.
The other messages (\texttt{tx\_req}, \texttt{tx\_resp} and \texttt{vd\_req}) are constant factors.
If we assume that every node performs transactions at a constant rate of $r_{\text{tx}}$ per second, then
$$l = r_{\text{tx}} D_{\text{c}},$$
where $D_{\text{c}}$ is the duration of a round of the consensus protocol.
But from \Cref{sec:cons-duration},
we know that $D_{\text{c}}$ is of $O(N)$; thus the communication cost per transaction is $O(N)$.
This result is intuitive because round duration would be longer if there are more CP blocks (more $N$), which means that the agreed fragments are longer (assuming nodes transact at a constant rate).
This behaviour is verified experimentally in \Cref{ch:implementation}.

\subsection{Linear global throughput}
\label{sec:global-throughput}
Using our results so far, we can analyse the global throughput.
First, we clarify the bandwidth definition, which is ``the data rate at which a network link or a network path can transfer''~\cite{prasad2003bandwidth}.
Now, suppose every node has $N$ links which connect to every other node,
every link has a fix bandwidth $C$ and every node makes $r_{\text{tx}}$ transactions per second.
Then we have the inequality
$$NC \ge r_{\text{tx}} l,$$
where $l$ is the length of the agreed fragment as before.
The inequality suggests that the rate for which transactions and validations are made cannot exceed the total bandwidth of all the links.

We note that the inequality does not hold if the node is only transacting with a subset of the population,
which is because it cannot use all the bandwidth available in all the links.
In the extreme case, if the node is only transacting with one other node, then it can only use the bandwidth of one link which is only $C$.
However, if that is the case, we can intelligently cache the \texttt{vd\_resp} messages as described in~\Cref{sec:caching}.
Hence, we analyse the worst case where every node transacts with a random node from the population,
and a new \texttt{vd\_resp} must be sent for every \texttt{vd\_req} message.

Consider the case where the system is making use of all the bandwidth, i.e. $NC = r_{\text{tx}} l$.
Recall that $l$ is of $O(N)$, which means LHS and RHS both grow linearly with respect to $N$.
Hence, there exists some $r_\text{tx}$ that makes use of all the available bandwidth regardless of $N$.
Finally, if every node in the network is transacting at $r_{\text{tx}}$ per second,
then the global throughput (in terms of transactions per second) is linear w.r.t. the population size $N$.
We also maintain a constant transaction rate by the same argument if the system is not making use of all the bandwidth.
Both claims are verified experimentally in~\Cref{ch:implementation}.

% The upshot of this analysis is that our system scales nicely at a global throughput of $O(N)$ until the population gets too large.
% Then we maintain a constant global throughput.
% This result falls a bit short of what we envisioned in the introduction.
% However, the population is not the number of nodes that use the system, but the number of nodes that are online during a single round.
% Furthermore, this result is for the worst case where every transaction needs an agreed fragment to be transmitted.
% In practice, nodes are able to cache agreed fragments.
% For instance, if $u$ and $v$ make $x$ transactions in a single round,
% then only one agreed fragment need to be exchanged as it contains all the transactions rather than $x$ agreed fragments.

\section{Effect of a highly adversarial environment}
\label{sec:highly-adversarial}

Our last study considers the effect when the number of adversaries is more than $t$.
This is useful because in practice it is difficult to guarantee that $t$ satisfies $n \ge 3t + 1$,
especially when $N$ is large.
Hence we are interested in the probability for this to happen under our facilitator election process.

The problem can be formulated as follows.
Suppose an urn contains $N$ balls, $t$ are black (malicious) and $N-t$ are white (honest).
If $n$ balls are drawn uniformly at random without replacement,
what is the probability that more than $\lfloor \frac{n-1}{3} \rfloor$ are black?
The random variable $X$, in this case, is the number of black balls, or the number of successful events.
It follows the hypergeometric distribution since we pick balls \emph{without} replacement~\cite{skala2013hypergeometric}.
Hence, we are interested in the following probability.
$$
1 - \sum_{k = 0}^{\lfloor \frac{n-1}{3} \rfloor} \Pr[X = k] = 
1 - \sum_{k = 0}^{\lfloor \frac{n-1}{3} \rfloor} \frac{ \binom{t}{k} \binom{N-t}{n-k} }{ \binom{N}{n} }
$$

This is not in closed form,
but we can visualise the effect in \Cref{fig:hypergeometric}.
We set the population size $N$ to 2000
and plot the probability of more than $\lfloor \frac{n-1}{3} \rfloor$ successful events for different numbers of draws.
Evidently,
if the number of black balls (traitors) is a third of the population (666 out of 2000)
we have about 0.5 probability of electing more than $\lfloor \frac{n-1}{3} \rfloor$ black balls for sufficiently large $n$.
Thus, we cannot expect the system to function correctly when 
the expected value is close to the number of black balls that we can tolerate.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{hypergeometric}
  \caption{Probability of selecting more than $\lfloor \frac{n-1}{3} \rfloor$ black balls for
  different value of $t$ with $N$ fixed at 2000.}
  \label{fig:hypergeometric}
\end{figure}

On the other hand, due to the fact that hypergeometric distributions have light tails with ``faster-than-exponential fall-off''~\cite{skala2013hypergeometric},
the probability for picking more than $\lfloor \frac{n-1}{3} \rfloor$ black balls when 
the expected value is much smaller than $\lfloor \frac{n-1}{3} \rfloor$ is small.
We can use tail inequality to bound the probability of picking more than $\lfloor \frac{n-1}{3} \rfloor$ black balls when only $n\alpha$ are black where $0 \le \alpha \le \lfloor \frac{n-1}{3} \rfloor / n$.
The tail inequality is
$$
\Pr[X \ge E[X] + \tau n] \le e^{-2\tau^2n},
$$
where $E[X] = n\alpha$.
We are interested in $\Pr[X \ge \lfloor \frac{n-1}{3} \rfloor + 1]$, so
$$
\tau = \frac{\lfloor \frac{n-1}{3} \rfloor + 1}{n} - \alpha
$$
Putting $\tau$ back into the tail inequality we get the following bound.
$$
\Pr[X \ge \lfloor \frac{n-1}{3} \rfloor + 1] \le e^{-2 \big(\frac{\lfloor  \frac{n - 1}{3} \rfloor + 1}{n} - \alpha \big)^2 n}
$$

The bound is not tight, but it is useful for picking parameters for a desired level of fault tolerance.
If $n$ is fixed, since $0 \le \alpha \le \lfloor \frac{n-1}{3} \rfloor/n < (\lfloor  \frac{n - 1}{3} \rfloor + 1) / n$,
the probability is maximum when the squared term is minimum at $\alpha = \lfloor \frac{n-1}{3} \rfloor/n$.
The probability is minimum when the squared term is maximum at $\alpha = 0$.
Hence, if $n$ is known, then we can pick a $\alpha$ such that the probability becomes small.
On the other hand, if $\alpha$ is fixed, but small, we may increase $n$ to achieve the same.
To put this into perspective,
suppose $n = 100$ and $\alpha = 1/10$, i.e. 10\% of the nodes are malicious,
then the probability to draw more black balls than the threshold is $9.9 \times 10^{-6}$.
